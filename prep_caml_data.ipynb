{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>the cardiomediastinal silhouette is within nor...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>both lungs are clear and expanded . heart and ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>there is xxxx increased opacity within the rig...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1002</td>\n",
       "      <td>status post left mastectomy . heart size norma...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Report ID                                               Text  \\\n",
       "0          1  the cardiac silhouette and mediastinum size ar...   \n",
       "1         10  the cardiomediastinal silhouette is within nor...   \n",
       "2        100  both lungs are clear and expanded . heart and ...   \n",
       "3       1000  there is xxxx increased opacity within the rig...   \n",
       "4       1002  status post left mastectomy . heart size norma...   \n",
       "\n",
       "                                               Label  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_caml_train = pd.read_csv(\"caml_train.csv\", header = [0])\n",
    "orig_caml_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>interstitial markings are diffusely prominent ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1007</td>\n",
       "      <td>trachea is midline . the cardiomediastinal sil...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>the heart is again mildly enlarged . mediastin...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1012</td>\n",
       "      <td>lung volumes are low . bibasilar consolidation...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1014</td>\n",
       "      <td>slightly enlarged heart . no effusions . no ed...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Report ID                                               Text  \\\n",
       "0       1001  interstitial markings are diffusely prominent ...   \n",
       "1       1007  trachea is midline . the cardiomediastinal sil...   \n",
       "2        101  the heart is again mildly enlarged . mediastin...   \n",
       "3       1012  lung volumes are low . bibasilar consolidation...   \n",
       "4       1014  slightly enlarged heart . no effusions . no ed...   \n",
       "\n",
       "                                               Label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "3  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_caml_test = pd.read_csv(\"caml_test.csv\", header = [0])\n",
    "orig_caml_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 3141, TEST: 786\n"
     ]
    }
   ],
   "source": [
    "num_train = len(orig_caml_train)\n",
    "num_test = len(orig_caml_test)\n",
    "print(f'TRAIN: {num_train}, TEST: {num_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above splits were the train and test for CAML data used in the original homework assignment. To replicate the CLINICALBERT study by Nwamake Imasogie, I am going to split these differently after creating a label based on the first item in the \"label\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SUBJECT ID', 'TEXT', 'VALUES'], dtype='object')\n",
      "   SUBJECT ID                                               TEXT  \\\n",
      "0           1  the cardiac silhouette and mediastinum size ar...   \n",
      "1           2  borderline cardiomegaly . midline sternotomy x...   \n",
      "2           3  no displaced rib fractures pneumothorax or ple...   \n",
      "3           4  there are diffuse bilateral interstitial and a...   \n",
      "4           5  the cardiomediastinal silhouette and pulmonary...   \n",
      "\n",
      "                                              VALUES  \n",
      "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, ...  \n",
      "4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "caml_notes_all = pd.concat([orig_caml_train, orig_caml_test])\n",
    "caml_notes_all = caml_notes_all.sort_values([\"Report ID\"]).reset_index(drop=True)\n",
    "caml_notes_all = caml_notes_all.rename(columns={'Report ID': 'SUBJECT ID', 'Text': 'TEXT', 'Label': 'VALUES'})\n",
    "print(caml_notes_all.columns)\n",
    "print(caml_notes_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Capture the first value in CAML, which is the label.\n",
    "caml_notes_all[\"LABEL\"] = caml_notes_all.VALUES.apply(lambda x: int(x[1]))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>VALUES</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>borderline cardiomegaly . midline sternotomy x...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>no displaced rib fractures pneumothorax or ple...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>there are diffuse bilateral interstitial and a...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>the cardiomediastinal silhouette and pulmonary...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT ID                                               TEXT  \\\n",
       "0           1  the cardiac silhouette and mediastinum size ar...   \n",
       "1           2  borderline cardiomegaly . midline sternotomy x...   \n",
       "2           3  no displaced rib fractures pneumothorax or ple...   \n",
       "3           4  there are diffuse bilateral interstitial and a...   \n",
       "4           5  the cardiomediastinal silhouette and pulmonary...   \n",
       "\n",
       "                                              VALUES  LABEL  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, ...      0  \n",
       "4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>borderline cardiomegaly . midline sternotomy x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>no displaced rib fractures pneumothorax or ple...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>there are diffuse bilateral interstitial and a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>the cardiomediastinal silhouette and pulmonary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT ID                                               TEXT  LABEL\n",
       "0           1  the cardiac silhouette and mediastinum size ar...      1\n",
       "1           2  borderline cardiomegaly . midline sternotomy x...      0\n",
       "2           3  no displaced rib fractures pneumothorax or ple...      1\n",
       "3           4  there are diffuse bilateral interstitial and a...      0\n",
       "4           5  the cardiomediastinal silhouette and pulmonary...      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml_notes_all = caml_notes_all.drop(columns = [\"VALUES\"])\n",
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>borderline cardiomegaly . midline sternotomy x...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>no displaced rib fractures pneumothorax or ple...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>there are diffuse bilateral interstitial and a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>the cardiomediastinal silhouette and pulmonary...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT ID                                               TEXT  LABEL  \\\n",
       "0           1  the cardiac silhouette and mediastinum size ar...      1   \n",
       "1           2  borderline cardiomegaly . midline sternotomy x...      0   \n",
       "2           3  no displaced rib fractures pneumothorax or ple...      1   \n",
       "3           4  there are diffuse bilateral interstitial and a...      0   \n",
       "4           5  the cardiomediastinal silhouette and pulmonary...      0   \n",
       "\n",
       "   ADMITTIME  DISCHTIME  \n",
       "0 2022-03-01 2022-04-15  \n",
       "1 2022-03-01 2022-04-15  \n",
       "2 2022-03-01 2022-04-15  \n",
       "3 2022-03-01 2022-04-15  \n",
       "4 2022-03-01 2022-04-15  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml_notes_all[\"ADMITTIME\"] = pd.to_datetime(\"2022-03-01\")\n",
    "caml_notes_all[\"DISCHTIME\"] = pd.to_datetime(\"2022-04-15\")\n",
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>borderline cardiomegaly . midline sternotomy x...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>no displaced rib fractures pneumothorax or ple...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>there are diffuse bilateral interstitial and a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>the cardiomediastinal silhouette and pulmonary...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT ID                                               TEXT  LABEL  \\\n",
       "0           1  the cardiac silhouette and mediastinum size ar...      1   \n",
       "1           2  borderline cardiomegaly . midline sternotomy x...      0   \n",
       "2           3  no displaced rib fractures pneumothorax or ple...      1   \n",
       "3           4  there are diffuse bilateral interstitial and a...      0   \n",
       "4           5  the cardiomediastinal silhouette and pulmonary...      0   \n",
       "\n",
       "   ADMITTIME  DISCHTIME  \n",
       "0 2022-03-01 2022-03-15  \n",
       "1 2022-03-01 2022-04-15  \n",
       "2 2022-03-01 2022-03-15  \n",
       "3 2022-03-01 2022-04-15  \n",
       "4 2022-03-01 2022-04-15  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml_notes_all.loc[caml_notes_all[\"LABEL\"] == 1, \"DISCHTIME\"] = pd.to_datetime(\"2022-03-15\")\n",
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above caml notes have artificially set dates with readmission < 30 days for sick people and > 30 days for non-sick people. However the head TA does not think it's worth splitting he values based on artificially inserted dates (I was suggesting using a multinomial distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text records need to be cleaned as follows:\n",
    "(a) make all words lowercase\n",
    "(b) remove punctuation\n",
    "(c) remove numbers and words that contain numbers\n",
    "(d) remove newline characters \\n and carriage returns \\r\n",
    "(e) tokenize the text\n",
    "(f) remove stop words\n",
    "(g) lemmatize the words (using part-of-speech tagging) so that the model performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on its parts of speechprint('Punctuation:', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 153 stopwords\n"
     ]
    }
   ],
   "source": [
    "# Here are the stopwords\n",
    "num_stopwords = len(stopwords.words('english'))\n",
    "print(f'There are {num_stopwords} stopwords')\n",
    "# print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The POS tags need to be convered so that they work in WordNet.\n",
    "def convert_tag(treebank_tag):\n",
    "    '''Convert Treebank tags to WordNet tags'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n' # if no match, default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method takes tokens (a list of cleaned tokens with stopwords removed\n",
    "and returns a list of lemma words (words that have been lemmatized).\n",
    "\n",
    "    Params:\n",
    "        tokens (list of strings): cleaned tokens with stopwords removed\n",
    "        \n",
    "    Return:\n",
    "        lemma_words (list of strings): lemmatized words\n",
    "''' \n",
    "\n",
    "def lemmatize(tokens):\n",
    " \n",
    "    # POS-tag your data before lemmatizing\n",
    "    tagged_words = pos_tag(tokens) # outputs list of tuples [('recent', 'JJ'),...]\n",
    "    \n",
    "    # Lemmatize using WordNet's built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    \n",
    "    # Lemmatize list of tuples, output a list of strings\n",
    "    for tupl in tagged_words:\n",
    "        lemma_words.append(wnl.lemmatize(tupl[0], convert_tag(tupl[1])))\n",
    "    \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This preprocessing an tokenization method makes text lowercase, removes punctuation,\n",
    "removes number and words that contain numbers, removes newline and carriage rturns,\n",
    "tokenizes the words, removes stop words, and then lemmatizes the tokens and returns\n",
    "the lemmatized words.\n",
    "\n",
    "Params:\n",
    "    text (string): full original, uncleaned text\n",
    "Returns:\n",
    "    lemmatized_tokens (list of strings): cleaned words\n",
    "'''\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "\n",
    "    # Make text lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    \n",
    "    # Remove numbers and words that contain numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Remove newline chars and carriage returns\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\r', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    word_tokens = word_tokenize(text) \n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Call the above lemmatizer function to perform lemmatization\n",
    "    lemmatized_tokens = lemmatize(tokens)\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cardiac',\n",
       " 'silhouette',\n",
       " 'mediastinum',\n",
       " 'size',\n",
       " 'within',\n",
       " 'normal',\n",
       " 'limit',\n",
       " 'pulmonary',\n",
       " 'edema',\n",
       " 'focal',\n",
       " 'consolidation',\n",
       " 'xxxx',\n",
       " 'pleural',\n",
       " 'effusion',\n",
       " 'evidence',\n",
       " 'pneumothorax',\n",
       " 'normal',\n",
       " 'chest',\n",
       " 'xxxxx']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_string = 'A list of words having 3 or 4 numbers and punctuation!'\n",
    "test_string = caml_notes_all.TEXT.loc[0]\n",
    "preprocess_and_tokenize(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably simply sanitize all of the data at once before splitting it into test / train / val sets. \n",
    "(The original article didn't do this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "num_notes = len(caml_notes_all)\n",
    "caml_notes_all[\"TEXT\"] = caml_notes_all.TEXT.apply(preprocess_and_tokenize)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3927\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[cardiac, silhouette, mediastinum, size, withi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[borderline, cardiomegaly, midline, sternotomy...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[displace, rib, fracture, pneumothorax, pleura...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[diffuse, bilateral, interstitial, alveolar, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[cardiomediastinal, silhouette, pulmonary, vas...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT ID                                               TEXT  LABEL  \\\n",
       "0           1  [cardiac, silhouette, mediastinum, size, withi...      1   \n",
       "1           2  [borderline, cardiomegaly, midline, sternotomy...      0   \n",
       "2           3  [displace, rib, fracture, pneumothorax, pleura...      1   \n",
       "3           4  [diffuse, bilateral, interstitial, alveolar, o...      0   \n",
       "4           5  [cardiomediastinal, silhouette, pulmonary, vas...      0   \n",
       "\n",
       "   ADMITTIME  DISCHTIME  \n",
       "0 2022-03-01 2022-03-15  \n",
       "1 2022-03-01 2022-04-15  \n",
       "2 2022-03-01 2022-03-15  \n",
       "3 2022-03-01 2022-04-15  \n",
       "4 2022-03-01 2022-04-15  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(caml_notes_all))\n",
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "caml_notes_all['TEXTSTRING'] = caml_notes_all['TEXT'].apply(' '.join)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>ADMITTIME</th>\n",
       "      <th>DISCHTIME</th>\n",
       "      <th>TEXTSTRING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[cardiac, silhouette, mediastinum, size, withi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>cardiac silhouette mediastinum size within nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[borderline, cardiomegaly, midline, sternotomy...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>borderline cardiomegaly midline sternotomy xxx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[displace, rib, fracture, pneumothorax, pleura...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>displace rib fracture pneumothorax pleural eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[diffuse, bilateral, interstitial, alveolar, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>diffuse bilateral interstitial alveolar opacit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[cardiomediastinal, silhouette, pulmonary, vas...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>cardiomediastinal silhouette pulmonary vascula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT ID                                               TEXT  LABEL  \\\n",
       "0           1  [cardiac, silhouette, mediastinum, size, withi...      1   \n",
       "1           2  [borderline, cardiomegaly, midline, sternotomy...      0   \n",
       "2           3  [displace, rib, fracture, pneumothorax, pleura...      1   \n",
       "3           4  [diffuse, bilateral, interstitial, alveolar, o...      0   \n",
       "4           5  [cardiomediastinal, silhouette, pulmonary, vas...      0   \n",
       "\n",
       "   ADMITTIME  DISCHTIME                                         TEXTSTRING  \n",
       "0 2022-03-01 2022-03-15  cardiac silhouette mediastinum size within nor...  \n",
       "1 2022-03-01 2022-04-15  borderline cardiomegaly midline sternotomy xxx...  \n",
       "2 2022-03-01 2022-03-15  displace rib fracture pneumothorax pleural eff...  \n",
       "3 2022-03-01 2022-04-15  diffuse bilateral interstitial alveolar opacit...  \n",
       "4 2022-03-01 2022-04-15  cardiomediastinal silhouette pulmonary vascula...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3927\n",
      "Longest sequence is:  138\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 0\n",
    "num_recs = len(caml_notes_all)\n",
    "print(num_recs)\n",
    "for i in range(num_recs):\n",
    "    this_len = len(caml_notes_all.TEXT.iloc[i])\n",
    "    if this_len > max_seq_len:\n",
    "        max_seq_len = this_len\n",
    "print('Longest sequence is: ', max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the head TA Wu Zhenbang suggested I not use artificial dates, I am going to drop those and just keep TEXT and LABEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXTSTRING</th>\n",
       "      <th>TEXTLIST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cardiac silhouette mediastinum size within nor...</td>\n",
       "      <td>[cardiac, silhouette, mediastinum, size, withi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>borderline cardiomegaly midline sternotomy xxx...</td>\n",
       "      <td>[borderline, cardiomegaly, midline, sternotomy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>displace rib fracture pneumothorax pleural eff...</td>\n",
       "      <td>[displace, rib, fracture, pneumothorax, pleura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>diffuse bilateral interstitial alveolar opacit...</td>\n",
       "      <td>[diffuse, bilateral, interstitial, alveolar, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>cardiomediastinal silhouette pulmonary vascula...</td>\n",
       "      <td>[cardiomediastinal, silhouette, pulmonary, vas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL                                         TEXTSTRING  \\\n",
       "0      1  cardiac silhouette mediastinum size within nor...   \n",
       "1      0  borderline cardiomegaly midline sternotomy xxx...   \n",
       "2      1  displace rib fracture pneumothorax pleural eff...   \n",
       "3      0  diffuse bilateral interstitial alveolar opacit...   \n",
       "4      0  cardiomediastinal silhouette pulmonary vascula...   \n",
       "\n",
       "                                            TEXTLIST  \n",
       "0  [cardiac, silhouette, mediastinum, size, withi...  \n",
       "1  [borderline, cardiomegaly, midline, sternotomy...  \n",
       "2  [displace, rib, fracture, pneumothorax, pleura...  \n",
       "3  [diffuse, bilateral, interstitial, alveolar, o...  \n",
       "4  [cardiomediastinal, silhouette, pulmonary, vas...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caml = caml_notes_all[[\"LABEL\", \"TEXTSTRING\", \"TEXT\"]].copy()\n",
    "df_caml = df_caml.rename(columns={'TEXT': 'TEXTLIST'})\n",
    "df_caml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caml.to_csv('clean_caml.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a vectorizer to convert all this data to the equivalent of embeddings. It would be interesting to see how many words we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words:  1868\n"
     ]
    }
   ],
   "source": [
    "# DOUBLE CHECK\n",
    "\n",
    "all_words = set()\n",
    "for x in range(len(df_caml)):\n",
    "    this_set = set(df_caml.TEXTLIST[x])\n",
    "    all_words.update(this_set)\n",
    "num_all_words = len(all_words)\n",
    "print('Total unique words: ', num_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "her_vect = CountVectorizer(max_features = 3000, tokenizer = preprocess_and_tokenize)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# For CAML we don't need so many features, and the tokenizer is not needed since the \n",
    "# data has already been processed\n",
    "my_vect = CountVectorizer(max_features = num_all_words)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXTSTRING</th>\n",
       "      <th>TEXTLIST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cardiac silhouette mediastinum size within nor...</td>\n",
       "      <td>[cardiac, silhouette, mediastinum, size, withi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>borderline cardiomegaly midline sternotomy xxx...</td>\n",
       "      <td>[borderline, cardiomegaly, midline, sternotomy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>displace rib fracture pneumothorax pleural eff...</td>\n",
       "      <td>[displace, rib, fracture, pneumothorax, pleura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>diffuse bilateral interstitial alveolar opacit...</td>\n",
       "      <td>[diffuse, bilateral, interstitial, alveolar, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>cardiomediastinal silhouette pulmonary vascula...</td>\n",
       "      <td>[cardiomediastinal, silhouette, pulmonary, vas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL                                         TEXTSTRING  \\\n",
       "0      1  cardiac silhouette mediastinum size within nor...   \n",
       "1      0  borderline cardiomegaly midline sternotomy xxx...   \n",
       "2      1  displace rib fracture pneumothorax pleural eff...   \n",
       "3      0  diffuse bilateral interstitial alveolar opacit...   \n",
       "4      0  cardiomediastinal silhouette pulmonary vascula...   \n",
       "\n",
       "                                            TEXTLIST  \n",
       "0  [cardiac, silhouette, mediastinum, size, withi...  \n",
       "1  [borderline, cardiomegaly, midline, sternotomy...  \n",
       "2  [displace, rib, fracture, pneumothorax, pleura...  \n",
       "3  [diffuse, bilateral, interstitial, alveolar, o...  \n",
       "4  [cardiomediastinal, silhouette, pulmonary, vas...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-d02cde4bd780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_caml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEXTSTRING\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# preprocessor = my_cool_preprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_caml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEXTLIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# # df_caml.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0;34m\"The truth value of a {0} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\".format(\n\u001b[0;32m-> 1555\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m             )\n\u001b[1;32m   1557\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "new_cv = CountVectorizer(df_caml.TEXTSTRING) # preprocessor = my_cool_preprocessor\n",
    "my_vector = new_cv.fit_transform(df_caml.TEXTLIST.values)\n",
    "# # df_caml.head()\n",
    "new_cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# I applied .astype(str) to fix the ValueError: np.nan is an invalid document, expected byte or unicode string.xc\n",
    "# create the vectorizer\n",
    "df_caml.TEXT = df_caml.TEXTLIST.apply(my_vect.fit_transform)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      (0, 0)\\t1\\n  (1, 13)\\t1\\n  (2, 8)\\t1\\n  (3, ...\n",
       "1      (0, 2)\\t1\\n  (1, 3)\\t1\\n  (2, 9)\\t1\\n  (3, 1...\n",
       "2      (0, 5)\\t1\\n  (1, 15)\\t1\\n  (2, 7)\\t1\\n  (3, ...\n",
       "3      (0, 15)\\t1\\n  (1, 4)\\t1\\n  (2, 25)\\t1\\n  (3,...\n",
       "4      (0, 4)\\t1\\n  (1, 22)\\t1\\n  (2, 21)\\t1\\n  (3,...\n",
       "Name: TEXTLIST, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caml.TEXT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caml.TEXT.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = caml_notes_all.TEXT\n",
    "y = caml_notes_all.LABEL\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.20, random_state = 44)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = 0.50, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) + len(X_test) + len(X_val) == len(caml_notes_all), 'Lengths incorrect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(caml_notes_all.LABEL) / len(caml_notes_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CAML dataset is much less imbalanced than the MIMIC-III notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set prevalence (n = {:d}):\".format(len(X_train)), \"{:.2f}%\".format((y_train.sum()/len(X_train))*100))\n",
    "print(\"Validation set prevalence (n = {:d}):\".format(len(X_val)), \"{:.2f}%\".format((y_val.sum()/len(X_val))*100))\n",
    "print(\"Test set prevalence (n = {:d}):\".format(len(X_test)), \"{:.2f}%\".format((y_test.sum()/len(X_test))*100))\n",
    "print(\"All samples (n = {:d})\".format(len(caml_notes_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is to use Bag of Words. We first have to preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First train using the X_train vocabulary. You have to \"unlist\" each set, hence the lambda function.\n",
    "my_vect.fit(X_train.apply(lambda x: ' '.join(x)))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we saw that the total vocabulary was 1868, so there will be words that have not been trained for in the validation and test sets. We can create vectors by passing the text into the vectorizer to get back counts. We then sum over the columns to get the term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix = my_vect.transform(X_train[y_train == 0].astype(str))\n",
    "pos_doc_matrix = my_vect.transform(X_train[y_train == 1].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tf = np.sum(neg_doc_matrix, axis = 0)\n",
    "pos_tf = np.sum(pos_doc_matrix, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the train and validation sets into numerical matrices as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = my_vect.transform(X_train.astype(str))\n",
    "X_valid_tf = my_vect.transform(X_val.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can build a simple predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "# Classifier\n",
    "clf = LogisticRegression(C = 0.0001, penalty = 'l2', random_state = 44)\n",
    "clf.fit(X_train_tf, y_train)\n",
    "model = clf\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the probability of being sick based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = model.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = model.predict_proba(X_valid_tf)[:,1]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_prob = pd.DataFrame([y_train[:10].values, y_train_preds[:10]]).transpose()\n",
    "df_training_prob.columns = ['Actual', 'Probability']\n",
    "df_training_prob = df_training_prob.astype({\"Actual\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use L1 normalization, we get all probability values to be exactly 0.5. And if we use L2 normalization, all values are below 0.5, meaning all predictions point to 0. This is problematic, and probably indicates that there is not enough data to perform the training. I find this a bit surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the top positive and negative words predicting whether a patient is sick or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 15))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Negative', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Positive', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = get_most_important_features(my_vect, clf, 50)\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very interesting to note that the unknown word 'xxxx' is such a strong negative indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no point trying to test the accuracy of the model yet since it is clearly terrible. It fails with both L1 and L2. We need to try another approach from what the author did. One possibility is instead to use tf-idf vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score  \n",
    "\n",
    "thresh = 0.46\n",
    "X_test_tf = my_vect.transform(X_test.astype(str)) # Get the output labels as separate variables.\n",
    "y_test_preds = model.predict_proba(X_test_tf)[:,1] # These will be probabiilities from logistic regression\n",
    "assert len(y_test_preds) == len(y_test), 'different lengths!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_test_preds > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recall = recall_score(y_test, y_preds, thresh)\n",
    "test_precision = precision_score(y_test, y_preds, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the values don't work. Let's try TF-IDF embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caml_notes_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_caml = caml_notes_all[[\"LABEL\", \"TEXT\"]].copy()\n",
    "tfidf_caml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_caml.TEXT\n",
    "y = tfidf_caml.LABEL\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.20, random_state = 44)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = 0.50, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First train using the X_train vocabulary. You have to \"unlist\" each set, hence the lambda function.\n",
    "tfidf_vectorizer.fit(X_train.apply(lambda x: ' '.join(x)))\n",
    "len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf_vectorizer.transform(X_train.astype(str))\n",
    "X_valid_tf = tfidf_vectorizer.transform(X_val.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C = 0.0001, penalty = 'l2', random_state = 44)\n",
    "clf.fit(X_train_tf, y_train)\n",
    "model = clf\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now calculate the probability of being sick based on the model\n",
    "y_train_preds = model.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = model.predict_proba(X_valid_tf)[:,1]\n",
    "y_valid_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_prob = pd.DataFrame([y_train[:10].values, y_train_preds[:10]]).transpose()\n",
    "df_training_prob.columns = ['Actual', 'Probability']\n",
    "df_training_prob = df_training_prob.astype({\"Actual\": int})\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_5 = df_training_prob[['Probability']] > 0.47\n",
    "over_5.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still not getting any values over 0.5!!!! No way to get any precision measurements. Not sure what is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = get_most_important_features(tfidf_vectorizer, clf, 50)\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that's interesting is that with the new vectorization, the words and their importance are in a different order. I used n-grams to see if that might improve things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.49\n",
    "X_test_tf = tfidf_vectorizer.transform(X_test.astype(str)) # Get the output labels as separate variables.\n",
    "y_test_preds = model.predict_proba(X_test_tf)[:,1] # These will be probabiilities from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_test_preds > thresh\n",
    "y_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY COMPUTING LOSSES WITH THESE NEW LOG REG THRESHOLDS, AND DOCUMENT THE THRESHOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like with less than 4000 records, logistic regression is simply too weak a model to identify meaning predictions. There is a slight issue of prevalence, with the prevalence being less than 50%. We can try balancing he dataset and see if that makes any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Preprocess all the text, but leave each doc as string, not list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Apply the preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n",
    "          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n",
    "          lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Create the vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count (classic BoW)\n",
    "vectorizer_bow = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "#### Tf-Idf (advanced variant of BoW)\n",
    "vectorizer_tfidf = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the train data to the vectorizer\n",
    "Transform corpus using vectorizer to produce X-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus = dtf_train[\"text_clean\"]\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier\n",
    "classifier = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])## train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)## test\n",
    "X_test = dtf_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    ## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prevalence = calc_prevalence(y_test)\n",
    "# auc_test = roc_auc_score(y_test, y_preds)\n",
    "\n",
    "# print('Train prevalence(n = %d): %.3f'%(len(y_train), train_prevalence))\n",
    "# print('Valid prevalence(n = %d): %.3f'%(len(y_valid), valid_prevalence))\n",
    "# print('Test prevalence(n = %d): %.3f'%(len(y_test), test_prevalence))\n",
    "# print('Train AUC:%.3f'%auc_train)\n",
    "# print('Valid AUC:%.3f'%auc_valid)\n",
    "# print('Valid AUC:%.3f'%auc_test)\n",
    "# print('Train recall:%.3f'%train_recall)\n",
    "# print('Valid recall:%.3f'%valid_recall)\n",
    "# print('Test recall:%.3f'%test_recall)\n",
    "# print('Train precision:%.3f'%train_precision)\n",
    "# print('Valid precision:%.3f'%valid_precision)\n",
    "# print('Test precision:%.3f'%test_precision)# Quickly throw metrics into a table format. It's easier to eyeball it for a side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_perf_metrics = pd.DataFrame([\n",
    "    [train_prevalence, valid_prevalence, test_prevalence],\n",
    "    [train_recall, valid_recall, test_recall], \n",
    "    [train_precision, valid_precision, test_precision],\n",
    "    [auc_train, auc_valid, auc_test]], columns=['Training', 'Validation', 'Test'])df_test_perf_metrics.rename(index={0:'Prevalence', 1:'Recall', 2:'Precision', 3:'AUC'}, \n",
    "                 inplace=True)df_test_perf_metrics.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
